* ACID
** Atomicity
   entrie transaction is either applied or rolled back
** Consistency
   the databse should always move from one consistent state to the next
** Isolation
   The results of a transaction are usually invisible to other transactions until the transaction is complete
** Durability
   a transaction’s changes are permanent
* Sql
 and or precedence should explicit by () 
 
 escape, escape character
 
 string is represent by ''

 limit x = limit 0,x

 uniDataon, used to union has same column tables, remove duplicate row
 
 union all, same as union, but not remove duplicate row

 driven table is decide by sql engine

 when use join, 
 - on .. and .., show joined table all row, not match conditon column is null
 - on .. where .., only match conditon row display
 
 insert into xx on duplicate <key> update <other key> = <value>

 insert ignore

 truncate table, delete all data keep structure

 show create table <t>, show table create statements

 alter table <xx>:
 - add, add column
 - drop, delete column
 - change, modify column

 copy table:
 - select * from, include schema and data
 - select * like, include schema
 
 create view <name> as <select>, create view

 when create union index, let more distinction column at start

 explain, show sql query explain

** function
   - concat, concat string
   - toupper
   - tolower
   - sqrt
   - avg
   - sum
   - min
   - max

* Transaction Isolation Level
  - Serializable: means can't read uncommited data, any query data by this transaction can't be update by another transaction, if insert data will affect this transaction query also be blocked.(hold read, write, range-read lock) 
  - Repeatable Read(mysql default): means can't read uncommited data and any query data by this transaction can't be update by another transaction. (hold read, write lock), maybe phantom read(means read other transcation insert new data)
  - Read Commited: means can read commited data when other transaction commit. (hold write lock)
  - Read Uncommited: can view the results of uncommited transcation. best performance but rarely reasonble in practice
  [[./images/transaction-isolation-level.png]]
* Mysql
  mysql -u <user> -p, connect mysql and drop in mysql shell
  load data infile <path> into table <table>, import csv data to table
  select <columns> into outfile <path> from <Table>, export csv data to file
  mysqldump -u <user> <database> [<table>] > <path>, backup database/table
  MyISAM don't support transcation, mix InnoDB and MyISAM table in one transcation can't rollback MyISAM table
  lock can acquire any time in transcation execute, but all lock is release when transcation finish
  innodb use next-key locking strategy avoid phantom read by locking gap
  #+BEING_SRC mysql
  UPDATE swap_test SET x=(@temp:=x), x = y, y = @temp; #used to swap column value
  CREATE USER <name>@<host> IDENTIFIED BY <password>
  GRANT ALL PRIVILEGES ON <db> TO <user>@<host> IDENTIFIED BY <password>
  #+END_SRC
** create user and grant permission
   #+BEGIN_SRC mysql
   CREATE USER '<name>'@'<allow access host>' IDENTIFIED BY '<password>';
   GRANT ALL ON <database>.<tables> TO '<name>'@'<allow access host>';
   #+END_SRC
** get column type
   SELECT COLUMN_TYPE FROM information_schema.COLUMNS WHERE TABLE_NAME = '' AND COLUMN_TYPE= '';
** record sql
*** For those blessed with MySQL >= 5.1.12, you can control this option globally at runtime:
    SET GLOBAL log_output = 'TABLE';
    SET GLOBAL general_log = 'ON';
    SET GLOBAL slow_query_log = 'ON';
    Take a look at the table mysql.general_log mysql.slow_log
*** If you prefer to output to a file instead of a table:
    SET GLOBAL log_output = "FILE"; 
    SET GLOBAL general_log_file = "/var/lib/mysql/logs/general.log";
    SET GLOBAL slow_query_log_file="/var/lib/mysql/logs/slow.log";
    SET GLOBAL general_log = 'ON';
    SET GLOBAL slow_query_log = 'ON';
** character set
   SHOW VARIABLES LIKE 'character_set_%';
   SHOW VARIABLES LIKE 'collation_%';
   SET NAMES 'utf8';
   when you create a database, it inherits from the server-wide character_set_server setting
   when you create a table, it inherits from the database
   when you create a column, it inherits from the table
   columns are the only place MySQL stores values, higher levels are only defaults
   when client conmmunitate with server, client -> server,  server convert character_set_client to character_set_connection, when server to client, server convert character_set_connection to character_set_result
** docker utf-8
   sudo docker run -d --restart=always -p 3306:3306 --name mysql -e MYSQL_ROOT_PASSWORD=ud6UomooxuiZa3h -e LANG=C.UTF-8 -d mysql:5.7 --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci 
** table size
   #+BEGIN_SRC mysql
   SELECT table_name AS `Table`, round(((data_length + index_length) / 1024 / 1024), 2) `Size in MB`  FROM information_schema.TABLES  WHERE table_schema = "";
   #+END_SRC

** change session transaction isolation level
  #+BEING_SRC mysql
  SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITED;
  #+END_SRC
** db status
   SHOW ENGINE INNODB STATUS
   SHOW FULL PROCESSLIST
** benchmark tools
*** Full Stack
    - ab, apcahe http server beanchmarking tool
    - http_load
    - JMeter
*** Single component
    - mysqlslap
    - MySQL Beanchmark Suite (sql-bench)
    - Super Smack
    - Database Test Suite
    - Percona's TPCC-MySQL Tool
    - sysbench
      prepare data: 
        sysbench /home/weili/work/public/sysbench/src/lua/oltp_read_only.lua --threads=4 --mysql-host=127.0.0.1 --mysql-user=root --mysql-password= --mysql-port=3306 --tables=10 --table-size=1000000 prepare
      test select:
        sysbench /home/weili/work/public/sysbench/src/lua/oltp_read_only.lua --threads=16 --events=0 --time=300 --mysql-host=127.0.0.1 --mysql-user=root --mysql-password= --mysql-port=3306 --tables=10 --table-size=1000000 --range_selects=off --db-ps-mode=disable --report-interval=1 run
      clean up:
        sysbench /home/weili/work/public/sysbench/src/lua/oltp_read_only.lua --threads=16 --events=0 --time=300 --mysql-host=127.0.0.1 --mysql-user=root --mysql-password= --mysql-port=3306 --tables=10 --table-size=1000000 --range_selects=off --db-ps-mode=disable --report-interval=1 cleanup
** profile tool
*** percona toolkit
    pt-query-digest < slow.log, analyze slow query log file
    pt-query-digest < tcmpdump, analyze query from tcpdump result
*** SHOW PROFILE
    SET profiling = 1; enable profiling in this session
    execute query then use SHOW PROFILES get execute duration table
    SHOW PROFILE FOR QUERY <number> get query detail
    if need sort by execute time, use blow sql:
    SELECT STATE, SUM(DURATION) AS Total_R, ROUND(100 * SUM(DURATION) / (SELECT SUM(DURATION) FROM INFORMATION_SCHEMA.PROFILING WHERE QUERY_ID = @query_id), 2) AS Pct_R, COUNT(*) AS Calls, SUM(DURATION) / COUNT(*) AS "R/Call" FROM INFORMATION_SCHEMA.PROFILING WHERE QUERY_ID = <query_id> GROUP BY STATE ORDER BY Total_R DESC;
*** SHOW STATUS
    FLUSH STATUS;
    SHOW STATUS WHERE Variable_name like 'Handler%' Or Variable_name like 'Created%'
*** performance schema
*** show global status
    mysqladmin ext -i1 | awk '/Queries/{q=$4-qp;qp=$4} /Threads_connected/{tc=$4} /Threads_running/{printf "%5d %5d %5d\n", q, tc, $4}'
*** show processlist
    mysql -e 'SHOW PROCESSLIST\G' | grep State: | sort | uniq -c | sort -rn
*** pt-stalk pt-mpmp pt-collect pt-summary pt-mysql-summary
    use to monitor metric and collect diagnostic data
    need install gdb and oprofile
*** strace
    strace -cfp $(pidof mysqld)
** schema online change
   use tool pt-online-schema-change 
** duplicate index found
   pt-duplicate-key-checker
** unused index
   INFORMATION_SCHEMA.INDEX_SATISTICS on mariadb
   pt-index-usage
** Table Maintenance
   use CHECK TABLE to check table status
   use REPAIR TABLE to fix corrupt tables
   use ANALYZE TABLE to update index statistics
   use OPTIMIZE TABLE to solve fragmentation issue
mysql protocl is half-duplex, either client send or server send, when server send data it on Sending data state, when is finish then unlock it lock resources 
** query states
   - sleep, thread is waiting for a new query from the client
   - query, the thread is either executing the query or sending the result back to the client
   - locked, the thread is waiting for a table lock to be granted at the server level
   - Analyzing and statistics, checking sotrage engine statistics and optimizing the query
   - Copying to tmp table [on disk], the thread is processing the query and copying results to a temporary table
   - Sorting result
   - Sending data, tcan mean several things: the thread might be sending data between stages of the query, generating the result set, or returing the result set to the client
** statis query analysis
   pt-query-advisor
** restore view create sql from frm file
   SELECT REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(
   SUBSTRING_INDEX(LOAD_FILE('/var/lib/mysql/world/Oceania.frm'),
   '\nsource=', −1), '\\_','\_'), '\\%','\%'), '\\\\','\\'), '\\Z','\Z'), '\\t','\t'),
   '\\r','\r'), '\\n','\n'), '\\b','\b'), '\\\"','\"'), '\\\'','\''),
   '\\0','\0') AS source;
** FULLTEXT MATCH
   use FULLTEXT INDEX create fulltext match index
   use match([columns]) agianst keyword to select match result
** get config file location
/usr/sbin/mysqld --verbose --help | grep -A 1 'Default options
** replication
*** create replication accounts
    CREATE USER 'repl'@'%.example.com' IDENTIFIED BY 'p4ssword';
    GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%.example.com';
    GRANT REPLICATION CLIENT ON *.* TO 'repl'@'%.example.com';
*** config master and replica
**** master
     log_bin = mysql-bin
     server_id = 10
     sync_master_info = 1 # after 5.5, has performance overhead but more safe on crash
**** slave
     log_bin = mysql-bin
     server_id = 2
     relay_log = /var/lib/mysql/mysql-reply-bin
     skip_slave_start
     log_slave_updates = 1 # can use this replica as a mater of other replicas
     read_only = 1
     innodb_flush_logs_at_trx_commit=1 # Flush every log write
     innodb_support_xa=1 # MySQL 5.0 and newer only
     innodb_safe_binlog # MySQL 4.1 only, roughly equivalent to innodb_support_xa
     sync_relay_log = 1 # after 5.5, has performance overhead but more safe on crash
     sync_relay_log_info = 1 # after 5.5, has performance overhead but more safe on crash
*** starting replica
    on slave run this statement
    CHANGE MASTER TO MASTER_HOST = 'server1', MASTER_USER='repl', MASTER_PASSWORD='p4ssword', MASTER_LOG_FILE='mysql-bin.000001', MASTER_LOG_POS=0;
    START SLAVE;
*** show status
    show master status;
    show slave status;
    show master logs;
    show binlog events in 'binlog.000001' from 123
** shard tool
   Hibernate Shards
   HiveDB
** mysql cluster
   NDB Cluster
   Clustrix
** load balance
   HAProxy
** backup
   logical backup (myqldump or select into outfile)
   raw backup(copy mysql file, innodb not work good on this way)
   binary log backup
   delete expiry days binlog 'PURGE MASTER LOGS BEFORE CURRENT_DATE - INTERVAL N DAY' or set expire_logs_days
   tools:
   - mysql enterprise backup
   - percona XtraBackup
   - mylvmbackup
   - Zmanda Recovery Manager
   - mydumper
   - mysqldump

* Postgresql
  grant permission has ALL/SELECT/INSERT/UPDATE?DELETE/RULE/REFERENCES/TRIGGER/CREATE/TEMPORARY/EXECUTE/USAGE
** default super login
   sudo -u postgres psql postgres
** command
   arguments:
   - -h, the host connect to
   - -U, user
   - -p, port
   
   #+BEGIN_SRC sql
   CREATE USER <name> WITH PASSWORD '<password>'; -- create user
   CREATE DATABASE <name>; -- create database
   GRANT ALL PRIVILEGES ON DATABASE <db name> to <user name>; -- grant access
   ALTER ROLE <user name> <role name>; -- change user role
   CREATE OR REPLACE VIEW <name> AS <sql>; -- create or update view
   SELECT <c1>, <c2>... rank() OVER( PARTITION BY <cN> ORDER BY <cM>) FROM <table> -- use window function
   CREATE INDEX <name> ON <table>(<column>); -- create index
   CREATE INDEX CONCURRENTLY <name> ON <table>(<columns>); -- create index without lock table
   CREATE UNIQUE INDEX <name> on <table (<columns>); -- create unique index
   \copy <sql> TO '<file.tsv>' -- extract data to tab delimited file
   \copy <sql> TO '<file.csv>' WITH (FORMAT CSV) -- extract data to csv delimited file
   \copy <sql> TO '<file.dat>' WITH (FORMAT "Binary") -- extract data to binary file
   \copy <table> FROM '<file.tsv>'
   \copy <table> FROM '<file.csv>' WITH CSV
   \copy <table> FROM '<file.data>' WITH BINARY
   \l -- list all databases
   \l+ -- list all databases with addtional information
   \d [table name] -- show schemas, table name is optional, if omit is show database schema 
   \d+  -- list all table with additional information
   \du -- show user and roles
   \dn -- list all schemas
   \df -- list all functions
   \c <dbname> -- connect to another database
   \q -- quit from shell
   \e -- text editor insider psql, will open your default editor
   \? -- show help
   SELECT pg_size_pretty(pg_database_size('<database>')); -- measuring database size
   SELECT pg_size_pretty(pg_relation_size('<table>')); -- measuring table size
   SELECT pg_size_pretty(pg_relation_size('<index>')); -- measuring index size
   SELECT pg_size_pretty(pg_total_relation_size('<table>')); -- measuring table with index size
   EXPLAIN <sql>;  -- explian sql generic form
   EXPLAIN ANALYZE <sql; -- explian sql analyze form
   SELECT sum(heap_blks_read) as heap_read, sum(heap_blks_hit) as heap_hit, (sum(heap_blks_hit) - sum(heap_blks_read)) / sum(heap_blks_hit) as ratio FROM pg_statio_user_tables; -- show cache hit
   SELECT relname, 100 * idx_scan / (seq_scan + idx_scan) percent_of_times_index_used, n_live_tup rows_in_table FROM pg_stat_user_tables WHERE (seq_scan + idx_scan) > 0 ORDER BY n_live_tup DESC; -- show index usage
   SELECT t.typname, e.enumlabel FROM pg_type t, pg_enum e WHERE t.oid = e.enumtypid; -- show avialable enums
   SELECT schemaname,relname,n_live_tup FROM pg_stat_user_tables ORDER BY n_live_tup DESC; -- count all tables row
   #+END_SRC
** function
   a window function is similar to group, but it keep row separate, so you can use other column not in group
** Utilities
   pd_dump is utility for back up your database
   #+BEGIN_SRC bash
   pg_dump <dbname> > <file.sql> # create plaintext dump
   pg_dump -Fc <dbname> > <file.back> # create compressed binary format
   pg_dump -Ft <dbname> > <file.tar> # tarball
   #+END_SRC
   pg_restore is utility for restore from backup
   #+BEGIN_SRC bash
   pg_restore -Fc <file.bak> # restore from compressed binary format, if db exists
   pg_restore -Ft <file.tar> # restore from tarball, if db exists
   pg_restore -Fc -C <file.bak> #restore from compressed binary format, if db not exist
   pg_restore -Ft -C <file.tar> # restore from tarball, if db exists, if db not exist
   #+END_SRC
   copy is utility for copy data into and out of tables, it support 3 format: 
   1. binary
   2. tab delimited
   3. csv delimited

** Datetypes
   - Date - Date only(2012-04-25)
   - Time - Time only(13:00:00.00)
   - Timestamp - Date and Time (2012-04-25 13:00:00.00)
   - Time with Timezone - Time only(13:00:00.00 PST)
   - Timestamp with TimeZone (2012-04-25 13:00:00.00 PST)
   - Interval - A span of time(4 days)
   interval can used in sql to calculate time, e.g. SELECT * FROM users WHERE created_at >= (now() - interval '1 month');

     
** HStore 
   HSore is a key value store with Postgres, but after 9.4 should use JSONB instead
   #+BEGIN_SRC sql 
   CREATE EXTENSION hstore; -- enable hstore
   CREATE TABLE <name> ( <other columns> attributes hstore); -- create hstore column
   SELECT <attributes>->'<field name>' FROM <table>; -- query from hstore
   #+END_SRC 
** Array
   column defined as arrays of variable length, array can be inbuilt typ or user-defined type or an enumerated type
   #+BEGIN_SRC sql 
   CREATE TABLE <table> ( <name> <type>[]); -- create array type column
   INSERT INTO <table> VALUES ('{"value1","value2"}'); -- insert array
   INSERT INTO <table> VALUES (ARRAY['value1','value2']); -- insert array
   SELECT <filed>[<index>] FROM <table>; -- select array element
   SELECT <filed>[<from>:<to>] FROM <table>; -- select array element
   UPDATE rock_band set members[2] = 'Waters' where name = 'Pink Floyd'; -- modify array
   UPDATE rock_band set members = '{"Mason", "Wright", "Gilmour"}' where name = 'Pink Floyd'; -- modify array
   select name from rock_band where 'Mason' = ANY(members); -- searching in array
   #+END_SRC 
** Enumerated Data Types
   #+BEGIN_SRC sql
   CREATE TYPE <name> AS ENUM ('value1', 'value2'); -- create enum type
   CREATE TABLE <name> (<column name> <previous created enum type name>); -- create table use enum type
   ALTER TYPE <name> ADD VALUE 'newvalue' AFTER '<value>'; -- add value to enum
   #+END_SRC
** CTEs(Common Table Expressions)
   #+BEGIN_SRC sql
   WITH <name> AS (<sql>); -- create cte
   #+END_SRC
** JSONB
   GIN INDEX will index every single column and key within JSONB document
   #+BEGIN_SRC sql
   CREATE TABLE <name> (<column name> JSONB); -- create table with column jsonb type
   CREATE INDEX <name> ON <table> USING GIN (<column>); -- create a GIN index on JSONB column
   SELECT <column>->'<key>' FROM <table>; -- extracting an attribute
   SELECT <column>->>'<key>' FROM <table>; -- extracting an attribute as text
   SELECT <column>->>'<key>' FROM <table> WHERE <column>->'key' ? 'value'; -- filter by key holds value
   #+END_SRC
* sqlite3
  #+BEGIN_SRC sql
  VACUUM; # flush wal and shm file to db file
  #+END_SRC

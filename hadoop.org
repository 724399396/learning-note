* basic
Map-reduce is split to job, job contain mutiple tasks, contain two types: map tasks and reduce tasks 

Yarn will schedule task to run, if task failed, it automatically rescheduled to a different node 

Input is split to fixed size, called input splits or splits, each split run a map task, this size can configure, but it is normal to set to HDFS block size(because this size is max single node can hold, avoid data transfer) 

Map task output is write to local disk instead of HDFS, because this is intermediate result, no need to store to HDFS to replicate, if result miss will rerun map task to generate 

Reduce task input is from all map task output, reduce task is write to HDFS 

Combiner function is between map and reduce, it is for optimize data transfter, it maybe called 0, 1 and more times 

Partitioner is used split map output, when multipe reducer exist, same key map output will process by one reducer 

Steaming api, mapper will receiver input line by line and output result to each line, reducer will receive map result(ordered, need goup by self), output result, use hadoop $HADOOP_HOME/share/hadoop/tools/lib/hadoop-steaming-*.jar -input –output –mapper –reducer –combiner to run 

Namenode data is store on ${dfs.namenode.name.dir}, the current directory contain fsimage, editlog, current edit fiel 

 fsimage contain check point information, but no datanode info, it will reconstructor when rebuild namenode memory from datanode 

Editlog is file edit info, it is transcation  

Edit_inprogress is current edit file(until submit transaction to generate edit log) 

 Secondarynode tell namenode sync inporgress file to edits, then fetch latest fsimage and edits file from namenode(http get), apply fsimage and edits in memory, generate a new fsimage, put back to namenode with .ckpt extension, namenode check it then rename to normal 

Datanode directory under ${dfs.datanode.data.dir}, HDFS block start with blk prefix, when number of block reach set size, will create a new subdirectory 

When namenode start, it reconstructor a new fsimage from latest image and edists, before this finish, it under safe mode, only read can perform 
* hdfs
FileSystem provide hdfs api 

FileSystem.get  provide kinds of method to get FileSystem 

FileSystem instance .open open a file, get FSDataInputStream, implement seek(), positionRead() 

FileSystem instance .create write or create a file, get FSDataOutputStream, only getPos, hdfs only support write to end of file, new file will create it parent directory 

FileSystem instance .mkdirs create directories 

FileSystem instance .listStatus get file status like user, group, replicate, modified time from path or paths with PathFilter 

FileSystem instance .globStatus get files use pattern and PathFilter 

FileSystem instance .delete delete file or directory 

Data read stream flow: client open will response by namenode, namenode return some first block to client, client use this node location directly read from datanode(this will be closest node to client0, when first block read to end, get next data block location from namemode, any error will report to namenode 

Data write stream flow: client create fille, namenode ensure permission and file not exist, return a FSDataOutputStream, write data will split to chunk, DataStreamer will consumer chunk data(replicate), when datanode ack, remove from queue, namenode record data location 

Replication 3 strategy, first replication is place to same rack different node, second is to different rack, third is place to same rack as second but another node, only one network transfer require 

Flush() not guarantee data is can read by other node, hflush() guarantee all node can read just writed data, but not guarantee data is write to disk, close file will execute hflush(), hsync() guarantee data write to disk 

Distcp used to parallel copy file between hdfs, distcp is implement by map reduce, parameters: -p preserved permission, time … , -update only update new file, -delete delete file not exist from source, -m set map parallel number, if cross hadoop version, use webhdfs protocol 
* yarn
Yet another resource negotiator, is used manager resource 

Yarn has two domain, resource manger, node manager 

Hadoop 1.x is MapReduce 1, configuration by taskscheduler and jobschedulers 

Hadoop 2.x is MapReduce 2, host on yarn 

Yarn has 3 run policy: 1) FIFO(first in first out), 2) Capacity 3) Fair Scheduler, can change in yarn-site.xml by yarn.resourcemanager.scheduler.class property 

On Capacity policy, there be a hierarchical queue structure, use yarn.scheduler.capacity.<queue-name>.<sub-property> to configure 
* I/O
Hadoop keep every file CRC, when read or write, it will check crc is corret 

When you want operate on corruption data, can ignore crc 

Default is 512 byte to generate a crc file with .crc extension 

Hadoop support DEFLATE, gzip, bzip2, LZO, LZ4, Snappy format compression and decompression, implement CompressionCodec 

CompressionCodec instance .createInputStream and .createOutputStream to get a compress/decompress stream 

CompressionCodecFactory can get correspond codec by file extension 

Haddop lib support native implement for deflate, gzip and bzip2, when compress/decompress, will auto use those native implement, property io.native.lib.available controller this 

If codec need call with many times, use CodecPool 

FileOutputFormat.setCompressOutput FileOutputFormat.setOutputCompressorClass control reduce output format, also controll by property mapreduce.output.fileoutputformat.compress, mapreduce.output.fileoutputformat.compress.codec 

Map result compress controll by mapreduce.map.output.compress, mapreduce.map.output.codec 

Writables is hadoop default serialization framework, serialization need match 4 purpose: compact, fast, extensible, interoperable, Avro is a framework to overcome limitation of Writable 

Writable interface, void write(DataOutput out), void readFields(DataInput in) 

All java primitive(except char) has Writable implement, VInt and VLong is variable length, first byte is signal, second byte is length, VInt can convert to VLong, Text is Writable implement for String, Text implement is base on utf8 and mutable 

BytesWritable, 4 bytes length + followed elem, it is mutable 

NullableWritable is singleton immutable, write/read no data 

ObjectWritable is general-purpose wrapper for: primitives, String, enum, Writable, null and arrays 

SequenceFile purpose is solve small file problem, (decrese name node memory, optimize for map reduce), SequenceFile.createWriter to write, Sequence.Reader instance overloaded next() method to read 

Property io.serializations default set writable and avro, can add other framework, like Protol Buffer, Thrift 

Hadoop fs –text can show sequence file in text format 

MapFile is sorted SequenceFile 

Avro datafile is better choice than sequenceFile 
* Map Reduce
MRUnit is a test framework for test Map Reduce 

MapDriver().withMapper.withInput.withOutput.runTest to test mapper, withOutput can call 0 or more times to set expect output  

ReduceDriver().withReducer().withInput().withOutput().runTest to test driver 

MiniDFSCluster, MiniMRCluster, MiniYARNCluster is for test prepare a mini distribute env 

Application id is application_<resource manager start timestamp>_<application counter, 0 based>, job id is job_<resource manager start timestamp>_<application counter>, task id is task_<resource manager start timestamp>_<application counter>_m/r_<task id>, task maybe 

Retry, so is id by attempt_<resource manager start timestamp>_<application counter>_m/r_<task id>_<retry times> 

Counter is help debug 

MRUnit withCounter use to test counter 

Yarn has a log collector, default is disable, enable by set yarn.log-aggregation-enable to true 

Job performance tip: 1) number of mappers (if mapper run seconds to finish, should decrese mapper size), 2) number of reducer 3) check can enable combiner, 4) compressing 5) custom serilization implement RawComparator 6) shuffle tweaks 

JobControl is used to control job execute time and other 

Apache Oozie is a frame work to control job workflow 

Job client get job from Resource manager, resource manager create a application, then job client store application to hdfs, then client submit job to resource manager, resource manager call node manager start container(MRAppMaster), master retrieve input splits from HDFS, then use split to start new container run map/reduce task 

OutputCommitter is used config job/task setup/commit/abort 

FileInputFormat, set file input location and split policy 

InputSplit get a array of file location, pass to application master, master use location to scheduler task for locallization 

CombineFileFormat is used for a large of small file as input 

NLineInputFormat let every map receive N line input 

SequenceFileInputFormat, BinaryInputFormat, MultipleInputFormat, DbInputFormat, TableInputFormat(HBase) 

Counter: 

Counter is used to analyze, counter is better than log to debug 

There be some built-in counter, e.g. number of bytes read, map input records 

User can defined custom counter, use enum field as key, context.getCounter(enum.field).inrement(), enum type will be group, field will be counter name, you can also create dynamic counter by string group and counter name 

Sort: 

TotalOrderPartitioner and InputSampler use to total sort 

Tools: 

ChainMapper, ChainReducer 

FieldSelectionMapReduce and FieldSelectionMapper, FieldSelectionReducer select field from input 

IntSumReducer, LongSumRecucer, sum key 

InverseMapper, inverse key and map 

MultithreadedMapper 

RegexMapper, count input match regex 
